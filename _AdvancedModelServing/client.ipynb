{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "fEmbeddings = open(\"../embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "\n",
    "def addCharInfo(Sentence):\n",
    "    for i, data in enumerate(Sentence):\n",
    "        chars = [c for c in data]\n",
    "        Sentence[i] = [data, chars]\n",
    "    return Sentence\n",
    "\n",
    "def createMatrices(sentence, word2Idx, case2Idx, char2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "\n",
    "\n",
    "    wordIndices = []\n",
    "    caseIndices = []\n",
    "    charIndices = []\n",
    "    for word, char in sentence:\n",
    "        wordCount += 1\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = unknownIdx\n",
    "            unknownWordCount += 1\n",
    "        charIdx = []\n",
    "        for x in char:\n",
    "            charIdx.append(char2Idx[x])\n",
    "        # Get the label and map to int\n",
    "        wordIndices.append(wordIdx)\n",
    "        caseIndices.append(getCasing(word, case2Idx))\n",
    "        charIndices.append(charIdx)\n",
    "    \n",
    "    return [wordIndices, caseIndices, charIndices]\n",
    "\n",
    "# 0-pads all words\n",
    "def padding(Sentence):    \n",
    "    maxlen = 52\n",
    "    char = Sentence[2]\n",
    "    for x in char:\n",
    "        maxlen = max(maxlen, len(x))\n",
    "    Sentence[0][2] = pad_sequences(Sentence[0][2], 52, padding='post')\n",
    "    return Sentence\n",
    "\n",
    "# define casing s.t. NN can use case information to learn patterns\n",
    "def getCasing(word, caseLookup):\n",
    "    casing = 'other'\n",
    "\n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "\n",
    "    digitFraction = numDigits / float(len(word))\n",
    "\n",
    "    if word.isdigit():  # Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower():  # All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper():  # All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "\n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDLT(object):\n",
    "    def embed(self, Sentence):\n",
    "\n",
    "        words = {}\n",
    "        for token, char in Sentence:\n",
    "            words[token.lower()] = True\n",
    "\n",
    "        # Map token cases\n",
    "        # PADDING_TOKEN: pad sentences to make them the same length\n",
    "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
    "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
    "\n",
    "        word2Idx = {}\n",
    "        self.wordEmbeddings = []\n",
    "        # Loop through words in embeddings\n",
    "        for line in fEmbeddings:\n",
    "            split = line.strip().split(\" \")\n",
    "            word = split[0]  # embedding word entry\n",
    "\n",
    "            if len(word2Idx) == 0:  # add padding+unknown\n",
    "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
    "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
    "\n",
    "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
    "\n",
    "        # Create string with possible characters\n",
    "        chars = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\"\n",
    "\n",
    "        # Create dictionary of possible characters\n",
    "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "        for c in chars:\n",
    "            self.char2Idx[c] = len(self.char2Idx)\n",
    "        \n",
    "        #self.fuckFuck = padding(createMatrices(Sentence, word2Idx, case2Idx, self.char2Idx))\n",
    "        self.embedded = createMatrices(Sentence, word2Idx, case2Idx, self.char2Idx)\n",
    "        flat = []\n",
    "        for sublist in self.embedded[2]:\n",
    "            for item in sublist:\n",
    "                flat.append(item)\n",
    "        self.embedded[2] = flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', ['I']],\n",
       " ['am', ['a', 'm']],\n",
       " ['from', ['f', 'r', 'o', 'm']],\n",
       " ['London', ['L', 'o', 'n', 'd', 'o', 'n']]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am from London'  # Python 3\n",
    "text = text.split()\n",
    "text = addCharInfo(text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddlt = DDLT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddlt.embed(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ddlt.embedded[0]\n",
    "while(len(words) < 52):\n",
    "    words.append(0)\n",
    "casings = ddlt.embedded[1]\n",
    "chars = ddlt.embedded[2]\n",
    "while(len(chars) < 52):\n",
    "    chars.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 13, 25, 18, 30, 27, 25, 50, 27, 26, 16, 27, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(str(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://10.0.75.2:8501/v1/models/my_model:predict'\n",
    "data = '''{{\n",
    "    \"instances\": [ \n",
    "        {{ \n",
    "            \"words\" : {words},\n",
    "            \"casing\" : {casings},\n",
    "            \"characters\" : {chars} \n",
    "        }} \n",
    "    ] \n",
    "}}'''.format(words=str(words), casings=str(casings), chars=str(chars))\n",
    "response = requests.post(url, data=data)\n",
    "#print(response)\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"error\": \"Input to reshape is a tensor with 1560 values, but the requested shape requires a multiple of 81120\\\\n\\\\t [[{{node Character_embedding_21/Reshape_1}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _output_shapes=[[?,?,52,30]], _device=\\\\\"/job:localhost/replica:0/task:0/device:CPU:0\\\\\"](Character_embedding_21/embedding_lookup, Character_embedding_21/Reshape_1/shape)]]\" }'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"instances\": [ \\n        { \\n            \"words\" : [3, 5, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n            \"casing\" : [2, 1, 1, 3],\\n            \"characters\" : [47, 13, 25, 18, 30, 27, 25, 50, 27, 26, 16, 27, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \\n        } \\n    ] \\n}'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
