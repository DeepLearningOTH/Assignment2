{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client\n",
    "## Known Issues:\n",
    "We can Serve the model to the server and can also send requests but we get an error-message that two shapes doesen't fit together. We think it's because the model in training uses glove but it won't be stored at the server after serving it.\n",
    "\n",
    "We tried hard to change this but after a couple of hours we gave up, noone knows how we can serve model so that glove will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fEmbeddings = open(\"../embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "\n",
    "def addCharInfo(Sentence):\n",
    "    for i, data in enumerate(Sentence):\n",
    "        chars = [c for c in data]\n",
    "        Sentence[i] = [data, chars]\n",
    "    return Sentence\n",
    "\n",
    "def createMatrices(sentence, word2Idx, case2Idx, char2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "\n",
    "\n",
    "    wordIndices = []\n",
    "    caseIndices = []\n",
    "    charIndices = []\n",
    "    for word, char in sentence:\n",
    "        wordCount += 1\n",
    "        if word in word2Idx:\n",
    "            wordIdx = word2Idx[word]\n",
    "        elif word.lower() in word2Idx:\n",
    "            wordIdx = word2Idx[word.lower()]\n",
    "        else:\n",
    "            wordIdx = unknownIdx\n",
    "            unknownWordCount += 1\n",
    "        charIdx = []\n",
    "        for x in char:\n",
    "            charIdx.append(char2Idx[x])\n",
    "        # Get the label and map to int\n",
    "        wordIndices.append(wordIdx)\n",
    "        caseIndices.append(getCasing(word, case2Idx))\n",
    "        charIndices.append(charIdx)\n",
    "    \n",
    "    return [wordIndices, caseIndices, charIndices]\n",
    "\n",
    "# 0-pads all words\n",
    "def padding(Sentence):    \n",
    "    maxlen = 52\n",
    "    char = Sentence[2]\n",
    "    for x in char:\n",
    "        maxlen = max(maxlen, len(x))\n",
    "    Sentence[0][2] = pad_sequences(Sentence[0][2], 52, padding='post')\n",
    "    return Sentence\n",
    "\n",
    "# define casing s.t. NN can use case information to learn patterns\n",
    "def getCasing(word, caseLookup):\n",
    "    casing = 'other'\n",
    "\n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "\n",
    "    digitFraction = numDigits / float(len(word))\n",
    "\n",
    "    if word.isdigit():  # Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower():  # All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper():  # All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "\n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepere the input: Make vektors and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDLT(object):\n",
    "    def embed(self, Sentence):\n",
    "\n",
    "        words = {}\n",
    "        for token, char in Sentence:\n",
    "            words[token.lower()] = True\n",
    "\n",
    "        # Map token cases\n",
    "        # PADDING_TOKEN: pad sentences to make them the same length\n",
    "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
    "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
    "\n",
    "        word2Idx = {}\n",
    "        self.wordEmbeddings = []\n",
    "        # Loop through words in embeddings\n",
    "        for line in fEmbeddings:\n",
    "            split = line.strip().split(\" \")\n",
    "            word = split[0]  # embedding word entry\n",
    "\n",
    "            if len(word2Idx) == 0:  # add padding+unknown\n",
    "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
    "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
    "\n",
    "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
    "\n",
    "        # Create string with possible characters\n",
    "        chars = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\"\n",
    "\n",
    "        # Create dictionary of possible characters\n",
    "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "        for c in chars:\n",
    "            self.char2Idx[c] = len(self.char2Idx)\n",
    "        \n",
    "        #self.fuckFuck = padding(createMatrices(Sentence, word2Idx, case2Idx, self.char2Idx))\n",
    "        self.embedded = createMatrices(Sentence, word2Idx, case2Idx, self.char2Idx)\n",
    "        flat = []\n",
    "        for sublist in self.embedded[2]:\n",
    "            for item in sublist:\n",
    "                flat.append(item)\n",
    "        self.embedded[2] = flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am from London.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['I', ['I']],\n",
       " ['am', ['a', 'm']],\n",
       " ['from', ['f', 'r', 'o', 'm']],\n",
       " ['London.', ['L', 'o', 'n', 'd', 'o', 'n', '.']]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text = input()  # Python 3\n",
    "text = text.split()\n",
    "text = addCharInfo(text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddlt = DDLT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddlt.embed(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ddlt.embedded[0]\n",
    "# tried to alling all vectors to the same lenght, didn't help\n",
    "# also pushed them in a higher array, we thought it's because the\n",
    "# models input\n",
    "wordsArr = []\n",
    "wordsArr.append(words)\n",
    "\n",
    "while(len(words) < 52):\n",
    "    words.append(0)\n",
    "casings = ddlt.embedded[1]\n",
    "while(len(casings) < 52):\n",
    "    casings.append(0)\n",
    "casingsArr = []\n",
    "casingsArr.append(casings)\n",
    "chars = ddlt.embedded[2]\n",
    "while(len(chars) < 52):\n",
    "    chars.append(0)\n",
    "charsArr = []\n",
    "charsArr.append(chars)\n",
    "#chars = np.reshape(ddlt.embedded[2], (-1, 52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 5, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(str(wordsArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[47, 13, 25, 18, 30, 27, 25, 50, 27, 26, 16, 27, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(str(charsArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(str(casingsArr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send request to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://10.0.75.2:8501/v1/models/my_model:predict'\n",
    "data = '''{{\n",
    "    \"instances\": [ \n",
    "        {{ \n",
    "            \"words\" : {words},\n",
    "            \"casing\" : {casings},\n",
    "            \"characters\" : {chars}\n",
    "        }} \n",
    "    ]\n",
    "}}'''.format(words=str(wordsArr), casings=str(casingsArr), chars=str(charsArr))\n",
    "response = requests.post(url, data=data)\n",
    "#print(response)\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The errror-message, we don't get behind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \"error\": \"ConcatOp : Dimensions of inputs should match: shape[0] = [1,1,52,100] vs. shape[1] = [1,1,52,8]\\\\n\\\\t [[{{node concatenate_1/concat}} = ConcatV2[N=3, T=DT_FLOAT, Tidx=DT_INT32, _output_shapes=[[?,?,138]], _device=\\\\\"/job:localhost/replica:0/task:0/device:CPU:0\\\\\"](embedding_2/embedding_lookup, embedding_3/embedding_lookup, Flatten/Reshape_2, Maxpool/ExpandDims/dim)]]\" }'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"instances\": [ \\n        { \\n            \"words\" : [[3, 5, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\\n            \"casing\" : [[2, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\\n            \"characters\" : [[47, 13, 25, 18, 30, 27, 25, 50, 27, 26, 16, 27, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\\n        } \\n    ]\\n}'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
