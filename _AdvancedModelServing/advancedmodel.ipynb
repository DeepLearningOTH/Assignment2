{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Djv1JVQ0NlLY"
   },
   "source": [
    "# Sequence Tagging (Named Entity Recognition)\n",
    "# Advanced Example: Bi-Directional LSTM + CRF\n",
    "\n",
    "\n",
    "## Accuracy f1:\n",
    "* Epoch 0: 0.5073 %\n",
    "* Epoch 10: 0.8664 %\n",
    "* Epoch 30: 0.9154 %\n",
    "\n",
    "\n",
    "## Used Tools:\n",
    "\n",
    "* Tensorflow\n",
    "* Keras\n",
    "* GloVe (Word Embedding)\n",
    "\n",
    "## Source:\n",
    "[Sequence Tagging with Tensorflow by Guillaume Genthial](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)\n",
    "\n",
    "## Full Model:\n",
    "\n",
    "<img src=\"screens/model.png\" alt=\"graph\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpKBw9k-yvsX"
   },
   "outputs": [],
   "source": [
    "# install keras if necessary\n",
    "# !pip install -q keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vFXYv2CfKm-c"
   },
   "source": [
    "## Load packages\n",
    "* numpy\n",
    "* keras\n",
    "* validation.py (helper class)\n",
    "* prepro.py (helper class)\n",
    "* tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "ORnzEnvOsIKg",
    "outputId": "9cad97ca-1da2-4f1e-bbf1-566ab6197f8a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from validation import compute_f1\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
    "    Flatten, concatenate\n",
    "from prepro import readfile, createBatches, createMatrices, iterate_minibatches, addCharInformation, padding\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import SGD, Nadam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gcM9d9SWV-8"
   },
   "source": [
    "## Parameters:\n",
    "* EPOCHS = 30 (Number of times the training data gets iterated through)\n",
    "* DROPOUT= 0.5 (Probability for a normal neuron to be dropped at one iteration)\n",
    "* DROPOUT_RECURRENT = 0.25 (Probability for a reccurent neuron to be dropped at one iteration)\n",
    "* LSTM_STATE_SIZE = 200 (Size of LSTM)\n",
    "* CONV_SIZE = 3 (Size of convolutional network)\n",
    "* LEARNING RATE = 0.0105 (Learning rate at start, decreases at time)\n",
    "* OPTIMIZER = Nadam() ([Recommended] Nesterov Adam optimizer: Adam optimizer with Nestrov momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7k6zDJEVQS_T"
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "\n",
    "EPOCHS = 30\n",
    "DROPOUT = 0.5 \n",
    "DROPOUT_RECURRENT = 0.25\n",
    "LSTM_STATE_SIZE = 200 \n",
    "CONV_SIZE = 3   \n",
    "LEARNING_RATE = 0.0105\n",
    "OPTIMIZER = Nadam()\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K._LEARNING_PHASE = tf.constant(0)\n",
    "K.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jijOgxyfkz5"
   },
   "source": [
    "## Open word embedding GloVe to respresent words as global vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPm75Jg1foIq"
   },
   "outputs": [],
   "source": [
    "fEmbeddings = open(\"../embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gH9-0l_g22Z"
   },
   "source": [
    "## Main Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Word level representation from characters embeddings\n",
    "<img src=\"screens/char_representation.png\" alt=\"graph\" width=\"400\"/>\n",
    "\n",
    "### 2. Bidirectional LSTM on top of word representation to extract contextual representation of each word\n",
    "<img src=\"screens/bi-lstm.png\" alt=\"graph\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "O_WwPMo3sIKx",
    "outputId": "83ad9219-66b2-4620-d061-a8c806ab2f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class initialised.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Initialise class\"\"\"\n",
    "\n",
    "class CNN_BLSTM(object):\n",
    "    \n",
    "    def __init__(self, EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER):\n",
    "        \n",
    "        # Set parameters\n",
    "        self.epochs = EPOCHS\n",
    "        self.dropout = DROPOUT\n",
    "        self.dropout_recurrent = DROPOUT_RECURRENT\n",
    "        self.lstm_state_size = LSTM_STATE_SIZE\n",
    "        self.conv_size = CONV_SIZE\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.optimizer = OPTIMIZER\n",
    "        \n",
    "    def loadData(self):\n",
    "      \n",
    "        # Use the readfile() method of prepro.py to load the data and add character information\n",
    "        self.trainSentences = readfile(\"../data/train.txt\")\n",
    "        self.devSentences = readfile(\"../data/dev.txt\")\n",
    "        self.testSentences = readfile(\"../data/test.txt\")\n",
    "\n",
    "    def addCharInfo(self):\n",
    "      \n",
    "        # Use the addCharInformation() method of prepro.py to format the data to: [['the', ['t', 'h', 'e'], 'O\\n']['EU', ['E', 'U'], 'B-ORG\\n'], ...]\n",
    "        self.trainSentences = addCharInformation(self.trainSentences)\n",
    "        self.devSentences = addCharInformation(self.devSentences)\n",
    "        self.testSentences = addCharInformation(self.testSentences)\n",
    "\n",
    "    def embed(self):\n",
    "        \"\"\"Create word- and character-level embeddings\"\"\"\n",
    "\n",
    "        labelSet = set()\n",
    "        words = {}\n",
    "\n",
    "        # Add unique words and labels of all data files to literals\n",
    "        for dataset in [self.trainSentences, self.devSentences, self.testSentences]:\n",
    "            for sentence in dataset:\n",
    "                for token, char, label in sentence:   \n",
    "                    labelSet.add(label)\n",
    "                    words[token.lower()] = True\n",
    "\n",
    "        # Map labels\n",
    "        self.label2Idx = {}\n",
    "        for label in labelSet:\n",
    "            self.label2Idx[label] = len(self.label2Idx)\n",
    "\n",
    "        # Map token cases\n",
    "        # PADDING_TOKEN: pad sentences to make them the same length\n",
    "        case2Idx = {'numeric': 0, 'allLower': 1, 'allUpper': 2, 'initialUpper': 3, 'other': 4, 'mainly_numeric': 5,\n",
    "                    'contains_digit': 6, 'PADDING_TOKEN': 7}\n",
    "        self.caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n",
    "\n",
    "        # Read GLoVE word embeddings\n",
    "        word2Idx = {}\n",
    "        self.wordEmbeddings = []\n",
    "\n",
    "        \n",
    "\n",
    "        # Loop through words in embeddings\n",
    "        for line in fEmbeddings:\n",
    "            split = line.strip().split(\" \")\n",
    "            word = split[0]  # embedding word entry\n",
    "\n",
    "            if len(word2Idx) == 0:  # add padding+unknown\n",
    "                word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.zeros(len(split) - 1)  # zero vector for 'PADDING' word\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "                word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "                vector = np.random.uniform(-0.25, 0.25, len(split) - 1)\n",
    "                self.wordEmbeddings.append(vector)\n",
    "\n",
    "            if split[0].lower() in words:\n",
    "                vector = np.array([float(num) for num in split[1:]])\n",
    "                self.wordEmbeddings.append(vector)  # word embedding vector\n",
    "                word2Idx[split[0]] = len(word2Idx)  # corresponding word dict\n",
    "\n",
    "        self.wordEmbeddings = np.array(self.wordEmbeddings)\n",
    "        \n",
    "        # Create string with possible characters\n",
    "        chars = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\"\n",
    "\n",
    "        # Create dictionary of possible characters\n",
    "        self.char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n",
    "        for c in chars:\n",
    "            self.char2Idx[c] = len(self.char2Idx)\n",
    "\n",
    "        # Use padding() and createMatrices() methods from prepro.py to format the data to: [[wordindices], [caseindices], \n",
    "        # [padded word indices], [label indices]]\n",
    "        # Create train set\n",
    "        self.train_set = padding(createMatrices(self.trainSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "        # Create dev set\n",
    "        self.dev_set = padding(createMatrices(self.devSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "        # Create test set\n",
    "        self.test_set = padding(createMatrices(self.testSentences, word2Idx, self.label2Idx, case2Idx, self.char2Idx))\n",
    "\n",
    "        self.idx2Label = {v: k for k, v in self.label2Idx.items()}\n",
    "        \n",
    "    def createBatches(self):\n",
    "        \"\"\"Create batches with createBatches() method in prepro.py\"\"\"\n",
    "        \n",
    "        # Create train batch\n",
    "        self.train_batch, self.train_batch_len = createBatches(self.train_set)\n",
    "        # Create dev batch\n",
    "        self.dev_batch, self.dev_batch_len = createBatches(self.dev_set)\n",
    "        # Create test batch\n",
    "        self.test_batch, self.test_batch_len = createBatches(self.test_set)\n",
    "        \n",
    "    def tag_dataset(self, dataset, model):\n",
    "        \"\"\"Tag data with numerical values\"\"\"\n",
    "        \n",
    "        correctLabels = []\n",
    "        predLabels = []\n",
    "        \n",
    "        for i, data in enumerate(dataset):\n",
    "            tokens, casing, char, labels = data\n",
    "            tokens = np.asarray([tokens])\n",
    "            casing = np.asarray([casing])\n",
    "            char = np.asarray([char])\n",
    "            pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "            pred = pred.argmax(axis=-1)  # Predict the classes\n",
    "            correctLabels.append(labels)\n",
    "            predLabels.append(pred)\n",
    "            \n",
    "        return predLabels, correctLabels\n",
    "    \n",
    "    def buildModel(self):\n",
    "        \"\"\"Model layers\"\"\"\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        # Character input\n",
    "        self.character_input = Input(shape=(None, 52,), name=\"Character_input\")\n",
    "        embed_char_out = TimeDistributed(\n",
    "            Embedding(len(self.char2Idx), 30, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name=\"Character_embedding\")(\n",
    "            self.character_input)\n",
    "\n",
    "        dropout = Dropout(self.dropout)(embed_char_out)\n",
    "\n",
    "        # Convolutional Neural Network\n",
    "        # TimeDistributed(): add additional time dimention\n",
    "        conv1d_out = TimeDistributed(Conv1D(kernel_size=self.conv_size, filters=30, padding='same', activation='tanh', strides=1), name=\"Convolution\")(dropout)\n",
    "        maxpool_out = TimeDistributed(MaxPooling1D(52), name=\"Maxpool\")(conv1d_out)\n",
    "        char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
    "        char = Dropout(self.dropout)(char)\n",
    "\n",
    "        # Word-level input\n",
    "        self.words_input = Input(shape=(None,), dtype='int32', name='words_input')\n",
    "        words = Embedding(input_dim=self.wordEmbeddings.shape[0], output_dim=self.wordEmbeddings.shape[1], weights=[self.wordEmbeddings],\n",
    "                          trainable=False)(self.words_input)\n",
    "\n",
    "        # Case-info input\n",
    "        self.casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "        casing = Embedding(output_dim=self.caseEmbeddings.shape[1], input_dim=self.caseEmbeddings.shape[0], \n",
    "                           weights=[self.caseEmbeddings],\n",
    "                           trainable=False)(self.casing_input)\n",
    "\n",
    "        # Concat & bi-LSTM\n",
    "        self.output = concatenate([words, casing, char])\n",
    "        self.output = Bidirectional(LSTM(self.lstm_state_size, \n",
    "                                    return_sequences=True, \n",
    "                                    dropout=self.dropout,                        # on input to each LSTM block\n",
    "                                    recurrent_dropout=self.dropout_recurrent     # on recurrent input signal\n",
    "                                   ), name=\"BLSTM\")(self.output)\n",
    "        self.output = TimeDistributed(Dense(len(self.label2Idx), activation='softmax'),name=\"Softmax_layer\")(self.output)\n",
    "        \n",
    "        # tensor input infos\n",
    "        #global tensor_input_info_words\n",
    "        #tensor_input_info_words = tf.saved_model.utils.build_tensor_info(words_input) \n",
    "        #global tensor_input_info_characters\n",
    "        #tensor_input_info_characters = tf.saved_model.utils.build_tensor_info(character_input)\n",
    "        #global tensor_input_info_casing\n",
    "        #tensor_input_info_casing = tf.saved_model.utils.build_tensor_info(casing_input)\n",
    "        # output\n",
    "        #global tensor_output_info\n",
    "        #tensor_output_info = tf.saved_model.utils.build_tensor_info(output)\n",
    "        \n",
    "        # tensor input infos\n",
    "        #tensor_input_info_words = words_input\n",
    "        #tensor_input_info_characters = character_input\n",
    "        #tensor_input_info_casing = casing_input\n",
    "        # output\n",
    "        #tensor_output_info = output\n",
    "        \n",
    "        # Set up model\n",
    "        self.model = Model(inputs=[self.words_input, self.casing_input, self.character_input], outputs=[self.output])\n",
    "        \n",
    "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=self.optimizer)\n",
    "        \n",
    "        self.init_weights = self.model.get_weights()\n",
    "        \n",
    "        \n",
    "    def save(self):\n",
    "        tf.saved_model.simple_save(\n",
    "            sess,\n",
    "            'save/1',\n",
    "            inputs={\n",
    "                'words': self.words_input, \n",
    "                'characters': self.character_input, \n",
    "                'casing': self.casing_input\n",
    "            },\n",
    "            outputs={\n",
    "                'entity_prediction' : self.output\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Default training\"\"\"\n",
    "        \n",
    "        # For plotting the learning curve\n",
    "        self.f1_test_history = []\n",
    "        self.f1_dev_history = []\n",
    "\n",
    "        # Run through defined epochs\n",
    "        for epoch in range(self.epochs):    \n",
    "                print(\"Epoch {}/{}\".format(epoch, self.epochs))\n",
    "                for i,batch in enumerate(iterate_minibatches(self.train_batch,self.train_batch_len)):\n",
    "                    labels, tokens, casing,char = batch\n",
    "                    self.model.train_on_batch([tokens, casing,char], labels)\n",
    "\n",
    "                # Compute test F1 scores\n",
    "                predLabels, correctLabels = self.tag_dataset(self.test_batch, self.model)\n",
    "                pre_test, rec_test, f1_test = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
    "                self.f1_test_history.append(f1_test)\n",
    "                print(\"   f1 test \", round(f1_test, 4))\n",
    "\n",
    "                # Compute dev F1 scrores\n",
    "                predLabels, correctLabels = self.tag_dataset(self.dev_batch, self.model)\n",
    "                pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, self.idx2Label)\n",
    "                self.f1_dev_history.append(f1_test)\n",
    "                print(\"   f1 dev \", round(f1_dev, 4), \"\\n\")\n",
    "\n",
    "            \n",
    "        print(\"Final F1 test score: \", f1_test)\n",
    "            \n",
    "        print(\"Training finished.\")\n",
    "\n",
    "    print(\"Class initialised.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHTiWtvzRR1e"
   },
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2184
    },
    "colab_type": "code",
    "id": "qfQAqz3QsILF",
    "outputId": "93910efa-8b68-403e-ce7f-0ee8ac7d9824",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "   f1 test  0.3435\n",
      "   f1 dev  0.3807 \n",
      "\n",
      "Final F1 test score:  0.34347587719298245\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Tensorboard setup\n",
    "tensorboard = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "# Create class\n",
    "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "# Load data\n",
    "cnn_blstm.loadData()\n",
    "# Add character information\n",
    "cnn_blstm.addCharInfo()\n",
    "# Create word and character embeddings\n",
    "cnn_blstm.embed()\n",
    "# Create batches\n",
    "cnn_blstm.createBatches()\n",
    "# Build network model\n",
    "cnn_blstm.buildModel()\n",
    "# Start training\n",
    "cnn_blstm.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: save/1\\saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "cnn_blstm.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFCTOOpXsILL"
   },
   "source": [
    "# Plot learning curve (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_4XIWM4PsILN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "8Sz1cMWksILR",
    "outputId": "d6a01304-9f95-4318-e856-c4db226c6664"
   },
   "outputs": [],
   "source": [
    "plt.plot(cnn_blstm.f1_test_history, label = \"F1 test\")\n",
    "plt.plot(cnn_blstm.f1_dev_history, label = \"F1 dev\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZ2FWBF5sILY"
   },
   "source": [
    "# Label distribution (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "-NTY89VUsILa",
    "outputId": "40fb80ee-ab81-475e-fd2c-257a8c600a79"
   },
   "outputs": [],
   "source": [
    "cnn_blstm = CNN_BLSTM(EPOCHS, DROPOUT, DROPOUT_RECURRENT, LSTM_STATE_SIZE, CONV_SIZE, LEARNING_RATE, OPTIMIZER)\n",
    "cnn_blstm.loadData()\n",
    "\n",
    "category_count = {\"B-ORG\\n\": 0, \"I-ORG\\n\":0, \"B-MISC\\n\": 0, \"I-MISC\\n\":0, \"B-LOC\\n\": 0, \"I-LOC\\n\": 0, \"B-PER\\n\": 0, \"I-PER\\n\": 0, \"O\\n\": 0}\n",
    "total_count = 0\n",
    "\n",
    "for sentence in cnn_blstm.trainSentences:\n",
    "    for word in sentence:\n",
    "        if word[1] in category_count.keys():\n",
    "            category_count[word[1]] += 1\n",
    "            total_count += 1\n",
    "\n",
    "for category, count in category_count.items():\n",
    "    print(\"{}: {}%\".format(category.replace(\"\\n\", \"\"), round((count/total_count)*100, 2)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nn_CoNLL.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
