{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simplemodel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5FX2nLGXbDoC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This programm will learn and execute sequence tagging based on LSTM with a following CNN-maxpooling-layer for feature selection.\n",
        "\n",
        "This programm depends on Tensorflow, Keras and Scikit-Learn version 0.20\n",
        "\n",
        "The original author of this code was:\n",
        "\n",
        "\n",
        "But the code was outdated due to scikit-learn >0.16.\n",
        "I updated those problems so it works again.\n",
        "Also the code was split in 3 python files for preprocessing, model and validation which I recombined in this file.\n",
        "\n",
        "Due to the fact that it's a simple model the f1score is not the best and only about 65%.\n",
        "But it's written in a simple way which is easy to understand and good for the process of learning how deeplearning works."
      ]
    },
    {
      "metadata": {
        "id": "crQp_Mbaa9kW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.initializers import RandomUniform\n",
        "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import Progbar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f-UJAabszSA7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define pathes to download needed data\n",
        "  # datasets\n",
        "path_train = 'https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/data/conll2003/en/train.txt'\n",
        "path_test = 'https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/data/conll2003/en/test.txt'\n",
        "path_valid = 'https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/data/conll2003/en/valid.txt'\n",
        "\n",
        "  # path to matrix which already contains a mapping from words to vectors. Can be extended locally\n",
        "path_glove = 'http://nlp.stanford.edu/data/glove.6B.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3NpqR8l9a9kc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Methods to preprocess Datasets"
      ]
    },
    {
      "metadata": {
        "id": "jtb1Hslqa9kd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define how the file with the input-content looks like, build sentences from the file, \n",
        "# per line there will be one sentence. Each word is tagged. The sentences\n",
        "# with later be the train-examples while the tagging will be the learned target\n",
        "\n",
        "def readfile(filename):\n",
        "    '''\n",
        "    read file\n",
        "    return format :\n",
        "    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n",
        "    '''\n",
        "    f = open(filename)\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        splits = line.split(' ')\n",
        "        sentence.append([splits[0],splits[-1]])\n",
        "\n",
        "    if len(sentence) >0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWqeQTC5a9kh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define the casing, so we have a word classification. Important words like\n",
        "# names, organization names, countries, towns \n",
        "# are normally written in initialUpperCase or allUpperCase\n",
        "# while for example verbs and adjectives in lowercase.\n",
        "\n",
        "# So the casing helps to get a better result because we have an important\n",
        "# additional classification\n",
        "\n",
        "def getCasing(word, caseLookup):   \n",
        "    casing = 'other'\n",
        "    \n",
        "    numDigits = 0\n",
        "    for char in word:\n",
        "        if char.isdigit():\n",
        "            numDigits += 1\n",
        "            \n",
        "    digitFraction = numDigits / float(len(word))\n",
        "    \n",
        "    if word.isdigit(): #Is a digit\n",
        "        casing = 'numeric'\n",
        "    elif digitFraction > 0.5:\n",
        "        casing = 'mainly_numeric'\n",
        "    elif word.islower(): #All lower case\n",
        "        casing = 'allLower'\n",
        "    elif word.isupper(): #All upper case\n",
        "        casing = 'allUpper'\n",
        "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
        "        casing = 'initialUpper'\n",
        "    elif numDigits > 0:\n",
        "        casing = 'contains_digit'\n",
        "    \n",
        "   \n",
        "    return caseLookup[casing]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQBm-a3va9kk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split the data into small batches, so the processing will be faster\n",
        "# there will be many text examples which could use a lot of memory so \n",
        "# splitting the data in batches is very useful\n",
        "\n",
        "def createBatches(data):\n",
        "    l = []\n",
        "    for i in data:\n",
        "        l.append(len(i[0]))\n",
        "    l = set(l)\n",
        "    batches = []\n",
        "    batch_len = []\n",
        "    z = 0\n",
        "    for i in l:\n",
        "        for batch in data:\n",
        "            if len(batch[0]) == i:\n",
        "                batches.append(batch)\n",
        "                z += 1\n",
        "        batch_len.append(z)\n",
        "    return batches,batch_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gSWt_zkua9ko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# here we preprocess the data, so we have the words (as unique numbers which can be processed),\n",
        "# the casing, and the word as a sequence of cased chars (which can be represented \n",
        "# as numbers and so they can be processed, too).\n",
        "# Also all labels will be represented in an array so the words can be classified correctly. \n",
        "\n",
        "# the output will be a matrix which contains the whole dataset of \n",
        "# processable numbers and chars and the label classification.\n",
        "\n",
        "def createMatrices(sentences, word2Idx, label2Idx, case2Idx,char2Idx):\n",
        "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
        "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
        "        \n",
        "    dataset = []\n",
        "    \n",
        "    wordCount = 0\n",
        "    unknownWordCount = 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        wordIndices = []    \n",
        "        caseIndices = []\n",
        "        charIndices = []\n",
        "        labelIndices = []\n",
        "        \n",
        "        for word,char,label in sentence:  \n",
        "            wordCount += 1\n",
        "            if word in word2Idx:\n",
        "                wordIdx = word2Idx[word]\n",
        "            elif word.lower() in word2Idx:\n",
        "                wordIdx = word2Idx[word.lower()]                 \n",
        "            else:\n",
        "                wordIdx = unknownIdx\n",
        "                unknownWordCount += 1\n",
        "            charIdx = []\n",
        "            for x in char:\n",
        "                charIdx.append(char2Idx[x])\n",
        "            #Get the label and map to int            \n",
        "            wordIndices.append(wordIdx)\n",
        "            caseIndices.append(getCasing(word, case2Idx))\n",
        "            charIndices.append(charIdx)\n",
        "            labelIndices.append(label2Idx[label])\n",
        "           \n",
        "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
        "        \n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7pbaK_yra9ks",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# the iterator will be defined now, so we can later iterate over the dataset\n",
        "# to learn every single data\n",
        "\n",
        "def iterate_minibatches(dataset,batch_len): \n",
        "    start = 0\n",
        "    for i in batch_len:\n",
        "        tokens = []\n",
        "        caseing = []\n",
        "        char = []\n",
        "        labels = []\n",
        "        data = dataset[start:i]\n",
        "        start = i\n",
        "        for dt in data:\n",
        "            t,c,ch,l = dt\n",
        "            l = np.expand_dims(l,-1)\n",
        "            tokens.append(t)\n",
        "            caseing.append(c)\n",
        "            char.append(ch)\n",
        "            labels.append(l)\n",
        "        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLC_3Eg_a9kv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def addCharInformation(Sentences):\n",
        "    for i,sentence in enumerate(Sentences):\n",
        "        for j,data in enumerate(sentence):\n",
        "            chars = [c for c in data[0]]\n",
        "            Sentences[i][j] = [data[0],chars,data[1]]\n",
        "    return Sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70TwonEza9kz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define a padding so each sentence will have the same length for processing the data\n",
        "\n",
        "def padding(Sentences):\n",
        "    maxlen = 52\n",
        "    for sentence in Sentences:\n",
        "        char = sentence[2]\n",
        "        for x in char:\n",
        "            maxlen = max(maxlen,len(x))\n",
        "    for i,sentence in enumerate(Sentences):\n",
        "        Sentences[i][2] = pad_sequences(Sentences[i][2],52,padding='post')\n",
        "    return Sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-yHV8tdta9k4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare Model and define common variables"
      ]
    },
    {
      "metadata": {
        "id": "aR1Rdy9ra9k5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define common variables\n",
        "\n",
        "epochs = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLUfZ-YKa9k-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define how to learn the tagging of the dataset\n",
        "\n",
        "def tag_dataset(dataset):\n",
        "    correctLabels = []\n",
        "    predLabels = []\n",
        "    b = Progbar(len(dataset))\n",
        "    for i,data in enumerate(dataset):    \n",
        "        tokens, casing,char, labels = data\n",
        "        tokens = np.asarray([tokens])     \n",
        "        casing = np.asarray([casing])\n",
        "        char = np.asarray([char])\n",
        "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
        "        pred = pred.argmax(axis=-1) #Predict the classes of labels           \n",
        "        correctLabels.append(labels)\n",
        "        predLabels.append(pred)\n",
        "        b.update(i)\n",
        "    return predLabels, correctLabels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W3BB7pzZa9lA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define how to get the train, test and validation data\n",
        "\n",
        "path_to_train_file = tf.keras.utils.get_file('train.txt', path_train)\n",
        "path_to_test_file = tf.keras.utils.get_file('test.txt', path_test)\n",
        "path_to_validation_file = tf.keras.utils.get_file('valid.txt', path_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdkyOuZaa9lE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check where the file is available\n",
        "\n",
        "path_to_train_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gNs_O6_aa9lI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read the files to get the raw content\n",
        "\n",
        "trainSentences = readfile(path_to_train_file)\n",
        "devSentences = readfile(path_to_validation_file)\n",
        "testSentences = readfile(path_to_test_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZG_-ufba9lM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainSentences = addCharInformation(trainSentences)\n",
        "devSentences = addCharInformation(devSentences)\n",
        "testSentences = addCharInformation(testSentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rYr5N6cga9lO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepare the labelSet and the sentences\n",
        "\n",
        "labelSet = set()\n",
        "words = {}\n",
        "\n",
        "for dataset in [trainSentences, devSentences, testSentences]:\n",
        "    for sentence in dataset:\n",
        "        for token,char,label in sentence:\n",
        "            labelSet.add(label)\n",
        "            words[token.lower()] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1sB22hoa9lS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# :: Create a mapping for the labels ::\n",
        "label2Idx = {}\n",
        "for label in labelSet:\n",
        "    label2Idx[label] = len(label2Idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OYdKTiN5a9lW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# :: Hard coded case lookup ::\n",
        "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
        "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgLOb7DOa9la",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# :: Read in word embeddings ::\n",
        "word2Idx = {}\n",
        "wordEmbeddings = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oe566ZP_a9li",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# prepared matrix which contains a mapping from the most common train_words to vectors to be faster here. Other words will be added locally.\n",
        "path_to_emb_file_zip = tf.keras.utils.get_file('glove.6B', path_glove)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9LUBVhMa9lm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# read the file which contains the prepared matrix, the file with 100d is chosen, \n",
        "# there are also files with 50d and 300d available in the zip \n",
        "# the only difference is the performance of the learner and this programm to read all vectors\n",
        "zip = zipfile.ZipFile(path_to_emb_file_zip)\n",
        "fEmbeddings = zip.open('glove.6B.100d.txt', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kUXha4Ea9lp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check if the file-reading is ok\n",
        "fEmbeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xt0jTsjSa9lw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# adapt the vector with the prepared matrix (mapping word->vector) to\n",
        "# our examples to have a more words to choose and classify which can \n",
        "# influence our model in both ways (making it better because we know more words\n",
        "# which can help with new sentences for example in the tests,\n",
        "# or making it worse because we have more words as we need)\n",
        "\n",
        "for line in fEmbeddings:\n",
        "    split = line.strip().split(b' ')\n",
        "    word = split[0]\n",
        "    \n",
        "    if len(word2Idx) == 0: #Add padding+unknown\n",
        "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
        "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
        "        wordEmbeddings.append(vector)\n",
        "        \n",
        "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
        "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
        "        wordEmbeddings.append(vector)\n",
        "\n",
        "    if split[0].lower() in words:\n",
        "        vector = np.array([float(num) for num in split[1:]])\n",
        "        wordEmbeddings.append(vector)\n",
        "        word2Idx[split[0]] = len(word2Idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WP34N08Ia9lz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#classify known vs. unknown token\n",
        "\n",
        "wordEmbeddings = np.array(wordEmbeddings)\n",
        "char2Idx = {\"PADDING_TOKEN\":0, \"UNKNOWN_TOKEN\":1}\n",
        "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
        "    char2Idx[c] = len(char2Idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IPM2tUQra9l1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word2Idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PjZ95eEBn-Sg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Real data preprocessing - from the dataset(sentences and labels)  to the processable matrices with paddings, and preparing the batches - starts now"
      ]
    },
    {
      "metadata": {
        "id": "Bf2awUcRa9l5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create the padding around our sentences in the datasets now\n",
        "\n",
        "train_set = padding(createMatrices(trainSentences,word2Idx,label2Idx,case2Idx,char2Idx))\n",
        "dev_set = padding(createMatrices(devSentences,word2Idx,label2Idx,case2Idx,char2Idx))\n",
        "test_set = padding(createMatrices(testSentences,word2Idx,label2Idx,case2Idx,char2Idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPrKsmQ7a9l9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create mapping position to label\n",
        "\n",
        "idx2Label = {v: k for k, v in label2Idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vb6jmINya9mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create the batches\n",
        "\n",
        "train_batch,train_batch_len = createBatches(train_set)\n",
        "dev_batch,dev_batch_len = createBatches(dev_set)\n",
        "test_batch,test_batch_len = createBatches(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43E6NsHuopsg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model will be defined now, the model-architecture will be shown\n",
        "This would be a perfect place for tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "0k4Z5dqva9mF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
        "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
        "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
        "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
        "character_input=Input(shape=(None,52,),name='char_input')\n",
        "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
        "dropout= Dropout(0.5)(embed_char_out)\n",
        "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
        "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
        "char = TimeDistributed(Flatten())(maxpool_out)\n",
        "char = Dropout(0.5)(char)\n",
        "output = concatenate([words, casing,char])\n",
        "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
        "output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)\n",
        "model = Model(inputs=[words_input, casing_input,character_input], outputs=[output])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B7_l1fcpo_SW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start training"
      ]
    },
    {
      "metadata": {
        "id": "iuryIaGna9mJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):    \n",
        "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
        "    a = Progbar(len(train_batch_len))\n",
        "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
        "        labels, tokens, casing,char = batch       \n",
        "        model.train_on_batch([tokens, casing,char], labels)\n",
        "        a.update(i)\n",
        "    print(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cXYHZWDypCjS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define Methods for getting any validation score, here the accuracy will be defined by the f1score via the precision and the recall"
      ]
    },
    {
      "metadata": {
        "id": "KmY48HRda9mR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_precision(guessed_sentences, correct_sentences):\n",
        "    assert(len(guessed_sentences) == len(correct_sentences))\n",
        "    correctCount = 0\n",
        "    count = 0\n",
        "    \n",
        "    \n",
        "    for sentenceIdx in range(len(guessed_sentences)):\n",
        "        guessed = guessed_sentences[sentenceIdx]\n",
        "        correct = correct_sentences[sentenceIdx]\n",
        "        assert(len(guessed) == len(correct))\n",
        "        idx = 0\n",
        "        while idx < len(guessed):\n",
        "            if guessed[idx][0] == 'B': #A new chunk starts\n",
        "                count += 1\n",
        "                \n",
        "                if guessed[idx] == correct[idx]:\n",
        "                    idx += 1\n",
        "                    correctlyFound = True\n",
        "                    \n",
        "                    while idx < len(guessed) and guessed[idx][0] == 'I': #Scan until it no longer starts with I\n",
        "                        if guessed[idx] != correct[idx]:\n",
        "                            correctlyFound = False\n",
        "                        \n",
        "                        idx += 1\n",
        "                    \n",
        "                    if idx < len(guessed):\n",
        "                        if correct[idx][0] == 'I': #The chunk in correct was longer\n",
        "                            correctlyFound = False\n",
        "                        \n",
        "                    \n",
        "                    if correctlyFound:\n",
        "                        correctCount += 1\n",
        "                else:\n",
        "                    idx += 1\n",
        "            else:  \n",
        "                idx += 1\n",
        "    \n",
        "    precision = 0\n",
        "    if count > 0:    \n",
        "        precision = float(correctCount) / count\n",
        "        \n",
        "    return precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZYJk7VLa9mO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Method to compute the accuracy. Call predict_labels to get the labels for the dataset\n",
        "def compute_f1(predictions, correct, idx2Label): \n",
        "    label_pred = []    \n",
        "    for sentence in predictions:\n",
        "        label_pred.append([idx2Label[element] for element in sentence])\n",
        "        \n",
        "    label_correct = []    \n",
        "    for sentence in correct:\n",
        "        label_correct.append([idx2Label[element] for element in sentence])\n",
        "            \n",
        "    \n",
        "    #print label_pred\n",
        "    #print label_correct\n",
        "    \n",
        "    prec = compute_precision(label_pred, label_correct)\n",
        "    rec = compute_precision(label_correct, label_pred)\n",
        "    \n",
        "    f1 = 0\n",
        "    if (rec+prec) > 0:\n",
        "        f1 = 2.0 * prec * rec / (prec + rec);\n",
        "        \n",
        "    return prec, rec, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcoAhyiMp043",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Get accuracy for test and validation datasets"
      ]
    },
    {
      "metadata": {
        "id": "-kUPRGNQa9mU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Predict labels for test dataset   \n",
        "predLabels, correctLabels = tag_dataset(test_batch)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZAJWRrsa9mY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the accuracy of test dataset\n",
        "\n",
        "pre_test, rec_test, f1_test= compute_f1(predLabels, correctLabels, idx2Label)\n",
        "print(\"Test-Data: Prec: %.3f%%, Rec: %.3f%%, F1: %.3f%%\" % (pre_test*100, rec_test*100, f1_test*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "01MBj1w_a9mb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#   Performance on validation dataset        \n",
        "predLabels, correctLabels = tag_dataset(dev_batch)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BwzlQA3Xa9mf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print the accuracy of validation dataset\n",
        "\n",
        "pre_dev, rec_dev, f1_dev = compute_f1(predLabels, correctLabels, idx2Label)\n",
        "print(\"Test-Data: Prec: %.3f%%, Rec: %.3f%%, F1: %.3f%%\" % (pre_dev*100, rec_dev*100, f1_dev*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}